{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "157fc795",
   "metadata": {},
   "source": [
    "# BM25 Document Search and Evaluation\n",
    "\n",
    "This Information Retrieval System allows users to index a corpus of documents and perform searches using a BM25 ranking model. The system is designed for processing large datasets efficiently by utilizing a SPIMI (Single Pass In-Memory Indexing) indexing technique.\n",
    "\n",
    "## Features\n",
    "\n",
    "- **Tokenizer**: Supports tokenization with options for case normalization, stopword removal, and stemming.\n",
    "- **Indexing**: Indexes documents in batches, allowing for scalability.\n",
    "- **Searching**: Implements the BM25 ranking model for retrieving relevant documents based on queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14dd669",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import ujson\n",
    "from typing import List, Dict\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path to import from src\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import our custom modules\n",
    "import src.data_processing as dp\n",
    "import src.bm25_search as bm25\n",
    "import src.evaluation as ndcg "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf515c1",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "The tokenizer processes text into tokens suitable for indexing and searching. It includes the following options:\n",
    "\n",
    "- **Case Normalization**: Converts all tokens to lowercase if `lowercase=True`.\n",
    "- **Minimum Token Length**: Discards tokens shorter than `min_token_length` (default is 3).\n",
    "- **Stopword Removal**: Removes common stopwords if a stopword list is provided.\n",
    "- **Stemming**: Reduces tokens to their stem forms using the Snowball Stemmer if `stem=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5005bb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths configuration\n",
    "CORPUS_PATH = '../data/MEDLINE_2024_Baseline.jsonl'\n",
    "OUTPUT_DIR = '../output'\n",
    "QUESTIONS_PATH = '../data/questions.jsonl'  # Optional: for batch processing\n",
    "INDEX_DIR = OUTPUT_DIR + '/index'\n",
    "\n",
    "# Check if input files exist\n",
    "if not Path(CORPUS_PATH).exists():\n",
    "    print(f\"Corpus file not found: {CORPUS_PATH}\")\n",
    "if not Path(QUESTIONS_PATH).exists():\n",
    "    print(f\"Questions file not found: {QUESTIONS_PATH}\")\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "Path(INDEX_DIR).mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Output directory ready: {INDEX_DIR}\")\n",
    "\n",
    "# Tokenizer configuration\n",
    "TOKENIZER_CONFIG = {\n",
    "    'min_token_length': 3,\n",
    "    'lowercase': True,\n",
    "    'stem': True,\n",
    "    'stopwords': None  # Can provide a set of stopwords\n",
    "}\n",
    "\n",
    "# BM25 parameters\n",
    "BM25_PARAMS = {\n",
    "    'k1': 1.2,  # Term frequency saturation\n",
    "    'b': 0.75   # Document length normalization\n",
    "}\n",
    "\n",
    "# Indexing parameters\n",
    "BATCH_SIZE = 10000\n",
    "N_RESULTS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916179be",
   "metadata": {},
   "source": [
    "\n",
    "## Corpus Indexing\n",
    "\n",
    "Run this section to build or select a existing index file. \n",
    "\n",
    "### Optimization Techniques in Indexing\n",
    "\n",
    "- **SPIMI Algorithm**: Implements the Single-Pass In-Memory Indexing algorithm to efficiently handle large datasets by writing partial indexes to disk.\n",
    "- **Batch Processing**: Indexes documents in batches (`batch_size=10000`) to manage memory usage.\n",
    "- **Precompiled Regular Expressions**: Uses precompiled regex patterns in the tokenizer to improve tokenization speed.\n",
    "- **Stemming Cache**: Caches stemmed tokens to avoid redundant computations during tokenization.\n",
    "- **MessagePack Serialization**: Uses `MessagePack` for efficient binary serialization when writing partial and merged indexes to disk.\n",
    "\n",
    "### Index File Format\n",
    "- **Format**: The index is stored as a MessagePack (`.msgpack`) file.\n",
    "\n",
    "- **Structure**:\n",
    "  - **Index**: A dictionary where keys are terms and values are dictionaries mapping document IDs to lists of positions where the term occurs.\n",
    "  - **Document Lengths**: A dictionary mapping document IDs to the total number of tokens in each document.\n",
    "\n",
    "The (`.msgpack`) file is a binary format, making it impossible to provide a screenshot of its contents. \n",
    "However, a sample from the file is shown below:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"index\": {\n",
    "    \"ethylhexyl\": {\n",
    "      \"25153068\": [7, 49],\n",
    "      \"32745991\": [3, 26],\n",
    "      \"12437285\": [8, 10],\n",
    "      \"15924484\": [5, 27],\n",
    "      \"19555962\": [150],\n",
    "      \"12270607\": [8, 29],\n",
    "      \"23356645\": [3, 22],\n",
    "      \"22041199\": [9, 17],\n",
    "      \"8333024\": [3, 25],\n",
    "      \"20453712\": [14, 22],\n",
    "      \"30960727\": [97],\n",
    "      \"37536456\": [13],\n",
    "      \"14687758\": [99],\n",
    "      \"35843048\": [21],\n",
    "      \"16956469\": [22],\n",
    "      \"34788783\": [1, 39, 50, 52, 83, 117, 265, 281],\n",
    "      \"14998748\": [14, 17, 28],\n",
    "      \"32610232\": [0, 48],\n",
    "      \"14556481\": [32],\n",
    "      \"35859238\": [35],\n",
    "      \"28661659\": [84, 92],\n",
    "      \"31033968\": [12],\n",
    "        .\n",
    "        .\n",
    "        .\n",
    "    },\n",
    "        .\n",
    "        .\n",
    "        .\n",
    "  },\n",
    "  \"doc_lengths\": {\n",
    "    \"2451706\": 115,\n",
    "    \"35308048\": 192,\n",
    "    \"7660250\": 51,\n",
    "    \"28963802\": 143,\n",
    "    \"25153068\": 231,\n",
    "    \"874026\": 101,\n",
    "    \"4001859\": 137,\n",
    "    \"10149271\": 92,\n",
    "    \"35267334\": 190,\n",
    "    \"3656477\": 128,\n",
    "    \"30818862\": 217,\n",
    "        .\n",
    "        .\n",
    "        .\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31d0c53",
   "metadata": {},
   "source": [
    "### Build Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c80e1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Building index from corpus: {CORPUS_PATH}\")\n",
    "print(f\"Output directory: {INDEX_DIR}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Tokenizer config: {TOKENIZER_CONFIG}\")\n",
    "    \n",
    "start_time = time.time()\n",
    "\n",
    "# Index documents\n",
    "index_path = dp.index_documents(\n",
    "    corpus_path=CORPUS_PATH,\n",
    "    output_dir=INDEX_DIR,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    tokenizer_config=TOKENIZER_CONFIG\n",
    ")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"\\nIndexing completed in {elapsed_time:.2f} seconds\")\n",
    "print(f\"Index saved at: {index_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc6820c",
   "metadata": {},
   "source": [
    "### Set Existing Index Path (if there's a `merged_index.msgpack` file already)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d5fa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_path = os.path.join(INDEX_DIR, 'merged_index.msgpack')\n",
    "print(f\"Using existing index: {index_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622f0906",
   "metadata": {},
   "source": [
    "## Load tokenizer configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0501006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer configuration\n",
    "print(\"Tokenizer configuration:\")\n",
    "for key, value in TOKENIZER_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e122f2b",
   "metadata": {},
   "source": [
    "## Batch Query Processing\n",
    "\n",
    "Process multiple queries from a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb572d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load queries from file\n",
    "def load_queries(file_path: str) -> List[Dict]:\n",
    "    \"\"\"Load queries from a JSONL file.\"\"\"\n",
    "    queries = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            queries.append(ujson.loads(line))\n",
    "    return queries\n",
    "\n",
    "# Process batch queries if file exists\n",
    "if os.path.exists(QUESTIONS_PATH):\n",
    "    print(f\"Loading queries from: {QUESTIONS_PATH}\")\n",
    "    queries = load_queries(QUESTIONS_PATH)\n",
    "    print(f\"Loaded {len(queries)} queries\")\n",
    "    \n",
    "    # Process all queries\n",
    "    start_time = time.time()\n",
    "    batch_results = bm25.batch_search(\n",
    "        queries=queries,\n",
    "        index_path=index_path,\n",
    "        tokenizer_config=TOKENIZER_CONFIG,\n",
    "        n_results=N_RESULTS,\n",
    "        k1=BM25_PARAMS['k1'],\n",
    "        b=BM25_PARAMS['b']\n",
    "    )\n",
    "    batch_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\nProcessed {len(batch_results)} queries in {batch_time:.2f} seconds\")\n",
    "    print(f\"Average time per query: {batch_time/len(batch_results):.3f} seconds\")\n",
    "    \n",
    "    # Save results\n",
    "    output_file = os.path.join(OUTPUT_DIR, 'ranked_questions.jsonl')\n",
    "    with open(output_file, 'w') as f:\n",
    "        for entry in batch_results:\n",
    "            f.write(ujson.dumps(entry) + '\\n')\n",
    "    \n",
    "    print(f\"\\nResults saved to: {output_file}\")\n",
    "else:\n",
    "    print(f\"Questions file not found at: {QUESTIONS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e6e6ce",
   "metadata": {},
   "source": [
    "## Evaluate Retrieved Documents\n",
    "\n",
    "Here we compute the **Normalized Discounted Cumulative Gain (nDCG)** metric, which evaluates the quality of the ranked retrieval results.\n",
    "\n",
    "### How nDCG Works\n",
    "\n",
    "- **DCG (Discounted Cumulative Gain)**: Measures the gain (relevance) of each document in the result list, discounted by its position in the list.\n",
    "- **IDCG (Ideal DCG)**: The maximum possible DCG achievable, obtained by an ideal ranking of documents.\n",
    "- **nDCG**: The ratio of DCG to IDCG, normalized to a value between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ed0233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute nDCG for the given results\n",
    "ndcg.compute_average_ndcg(\n",
    "    questions_file_path=QUESTIONS_PATH,\n",
    "    results_file_path=output_file,\n",
    "    k=10\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
