{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb9a2f34",
   "metadata": {},
   "source": [
    "# Neural Reranker \n",
    "\n",
    "## Overview \n",
    "\n",
    "The Neural Reranker and Evaluation System enhances the baseline Information Retrieval (IR) system by introducing a CNN-based neural reranker model to improve the relevance of retrieved documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c54987b",
   "metadata": {},
   "source": [
    "## Imports & Config\n",
    "\n",
    "Here we specify all the necessary modules, the required files and paths:\n",
    "- **Pretrained Embeddings**: Path to the pretrained embeddings file (txt format).\n",
    "- **Corpus File**: Path to the corpus file (JSONL format).\n",
    "- **Questions File**: Path to the questions file (JSONL format).\n",
    "- **Questions Ranked (BM25)**: Path to the BM25-ranked file (JSONL format).\n",
    "- **Training Data**: Path to the questions training file (JSONL format). \n",
    "- **BM25 Ranked Questions Training Data**: Path to the BM25-ranked training file (JSONL format).\n",
    "- **Ouput File**: Path to save the reranked results (JSONL format).\n",
    "- **Model Checkpoint**: Path to the trained model checkpoint (train new model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51809637",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import ujson\n",
    "from datetime import datetime \n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Add parent directory to path to import from src\n",
    "sys.path.append('..')\n",
    "\n",
    "import src.evaluation as ndcg \n",
    "from src.model import CNNInteractionBasedModel\n",
    "from src.tokenizer import Tokenizer\n",
    "from src.dataset import PointWiseDataset\n",
    "from src.data_processing import load_tokenizer_config\n",
    "from src.utils import (\n",
    "    load_pretrained_embeddings, \n",
    "    build_collate_fn, \n",
    "    get_all_doc_texts, \n",
    "    get_questions, \n",
    ")\n",
    "\n",
    "TOKENIZER_CONFIG = {\n",
    "    'min_token_length': 3,\n",
    "    'lowercase': True,\n",
    "    'stem': True,\n",
    "    'stopwords': None  # Can provide a set of stopwords\n",
    "}\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "OUTPUT_DIR = \"../output\"\n",
    "PRETRAINED_EMB = \"../data/glove.42B.300d.txt\"\n",
    "CORPUS_FILE = \"../data/MEDLINE_2024_Baseline.jsonl\"\n",
    "QUESTIONS_FILE = \"../data/questions.jsonl\"\n",
    "BM25_FILE = \"../data/questions_bm25_ranked.jsonl\"\n",
    "TRAIN_Q_FILE = \"../data/training_data.jsonl\"\n",
    "TRAIN_BM25_FILE = \"../data/training_data_bm25_ranked.jsonl\"\n",
    "BM25_OUPUT_FILE = \"../output/ranked_questions.jsonl\"\n",
    "OUTPUT_FILE = \"../output/final_ranked_questions.jsonl\"\n",
    "MODEL_CHECKPOINT = \"../output/model\"\n",
    "\n",
    "Path(MODEL_CHECKPOINT).mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Output model directory ready: {MODEL_CHECKPOINT}\")\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7489b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer \n",
    "\n",
    "config_path = os.path.join(OUTPUT_DIR, \"index/tokenizer_config.msgpack\")\n",
    "\n",
    "if os.path.exists(config_path):\n",
    "    tokenizer_config = load_tokenizer_config(config_path)\n",
    "    print(\"Loaded tokenizer configuration:\")\n",
    "    for key, value in tokenizer_config.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "else:\n",
    "    tokenizer_config = TOKENIZER_CONFIG\n",
    "    print(\"Using default tokenizer configuration\")\n",
    "\n",
    "tokenizer = Tokenizer(\n",
    "    tokenizer_config.get('min_token_length', 3),\n",
    "    tokenizer_config.get('lowercase', True),\n",
    "    tokenizer_config.get('stem', False),\n",
    "    set(tokenizer_config.get('stopwords', [])) if tokenizer_config.get('stopwords') else None\n",
    ")\n",
    "questions = get_questions(TRAIN_Q_FILE)\n",
    "documents = get_all_doc_texts(TRAIN_Q_FILE, TRAIN_BM25_FILE, CORPUS_FILE)\n",
    "tokenizer.fit(questions + documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606b0b9f",
   "metadata": {},
   "source": [
    "## Dataset and DataLoader\n",
    "The `PointWiseDataset` and `DataLoader` classes encapsulate the process of pulling your data from storage and exposing it to your training loop in batches.\n",
    "\n",
    "The `Dataset` is responsible for accessing and processing single instances of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d829f639",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading training dataset...\")\n",
    "train_dataset = PointWiseDataset(TRAIN_Q_FILE, TRAIN_BM25_FILE, CORPUS_FILE, tokenizer, use_negative_sampling=True)\n",
    "\n",
    "print(\"Loading validation dataset...\")\n",
    "validation_dataset = PointWiseDataset(QUESTIONS_FILE, BM25_FILE, CORPUS_FILE, tokenizer)\n",
    "\n",
    "# Determine max lengths\n",
    "q_lens = [len(train_dataset[i][\"question_token_ids\"]) for i in range(len(train_dataset))]\n",
    "d_lens = [len(train_dataset[i][\"document_token_ids\"]) for i in range(len(train_dataset))]\n",
    "max_q_len = max(3, max(q_lens))\n",
    "max_d_len = max(3, max(d_lens))\n",
    "print(f\"Max Q length: {max_q_len}, Max D length: {max_d_len}\")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=build_collate_fn(tokenizer, max_q_len, max_d_len),\n",
    "    shuffle=True,\n",
    "    pin_memory=(DEVICE.type == 'cuda')\n",
    ")\n",
    "\n",
    "validation_loader = DataLoader(\n",
    "    validation_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=build_collate_fn(tokenizer, max_q_len, max_d_len),\n",
    "    pin_memory=(DEVICE.type == \"cuda\")\n",
    ")   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0e44ab",
   "metadata": {},
   "source": [
    "## Training Procedure\n",
    "\n",
    "We train the reranking model in a pointwise classification setup, where the model learns to predict whether a retrieved document is **relevant (positive)** or **not relevant (negative)** to a given query.\n",
    "\n",
    "**1. Input Data**\n",
    "- Each **query** comes with a **set of retrieved documents** (from BM25).\n",
    "- Each (query, document) pair is labeled:\n",
    "    - **Positive (label=1)**: if the document is listed in the **gold-standard set** for that query.\n",
    "    - **Negative (label=0)**: otherwise.\n",
    "\n",
    "This produces training samples of the form:\n",
    "\n",
    "```sh\n",
    "(query_text, document_text, label)\n",
    "```\n",
    "\n",
    "**2. Negative/Positive Ratio**\n",
    "\n",
    "- Since BM25 retrieves many more non-relevant documents than relevant ones, the dataset is **highly imbalanced**.\n",
    "- To counter this, we use a **negative sampling strategy**:\n",
    "- For every **positive document**, we sample up to **k negatives** (e.g., 2× more negatives).\n",
    "    - This yields an approximate **1:2 ratio** of positives to negatives.\n",
    "    - This prevents the model from being biased toward always predicting \"non-relevant\".\n",
    "\n",
    "**3. Negative Mining Approach**\n",
    "\n",
    "We start with **BM25 retrieval** as a source of candidate documents:\n",
    "\n",
    "- Positives are guaranteed to be included (if retrieved).\n",
    "- Negatives are chosen from the **top BM25 results that are not in the gold standard**.\n",
    "    - These negatives are **“hard negatives”**, because they were highly ranked by BM25 but are not relevant.\n",
    "    - Training on these improves discrimination compared to random negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e73716",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, epochs=1, lr=1e-3):\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_vloss = 1_000_000.\n",
    "\n",
    "    model = model.to(DEVICE)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print('EPOCH {}:'.format(epoch + 1))\n",
    "        model.train(True)\n",
    "\n",
    "        running_loss = 0.\n",
    "        last_loss = 0.\n",
    "\n",
    "        for i, data in enumerate(train_loader):\n",
    "            qids = data[\"question_token_ids\"].to(DEVICE, non_blocking=True)\n",
    "            dids = data[\"document_token_ids\"].to(DEVICE, non_blocking=True)\n",
    "            labels = data[\"label\"].to(DEVICE, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            scores = model(qids, dids)\n",
    "            loss = loss_fn(scores, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if i % 1000 == 999:\n",
    "                last_loss = running_loss / 1000\n",
    "                print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "                running_loss = 0.\n",
    "\n",
    "        running_vloss = 0.0\n",
    "        # Set the model to evaluation mode, disabling dropout and using population\n",
    "        # statistics for batch normalization.\n",
    "        model.eval()\n",
    "\n",
    "        reranked_results = {}\n",
    "\n",
    "        # Disable gradient computation and reduce memory consumption.\n",
    "        with torch.no_grad():\n",
    "            for i, vdata in enumerate(validation_loader):\n",
    "                q_tokens = vdata[\"question_token_ids\"].to(DEVICE, non_blocking=True)\n",
    "                d_tokens = vdata[\"document_token_ids\"].to(DEVICE, non_blocking=True)\n",
    "                labels = vdata[\"label\"].to(DEVICE, non_blocking=True)\n",
    "                qids = vdata[\"query_ids\"]\n",
    "                dids = vdata[\"document_ids\"]\n",
    "\n",
    "                voutputs = model(q_tokens, d_tokens)\n",
    "                vloss = loss_fn(voutputs, labels)\n",
    "                running_vloss += vloss.item()\n",
    "\n",
    "                scores = torch.sigmoid(voutputs).cpu().numpy()\n",
    "\n",
    "                for i, qid in enumerate(qids):\n",
    "                    if qid not in reranked_results:\n",
    "                        reranked_results[qid] = []\n",
    "                    reranked_results[qid].append((dids[i], float(scores[i])))\n",
    "\n",
    "        avg_vloss = running_vloss / (i + 1)\n",
    "        print('LOSS train {} valid {}'.format(last_loss, avg_vloss))\n",
    "\n",
    "        # sort by model score\n",
    "        for qid, doc_scores in reranked_results.items():\n",
    "            reranked_results[qid] = [doc for doc, _ in sorted(doc_scores, key=lambda x: x[1], reverse=True)]\n",
    "\n",
    "        validation_file = os.path.join(OUTPUT_DIR, 'validation_ranked_questions_model.jsonl')\n",
    "        with open(validation_file, 'w') as f:\n",
    "            for qid, docs in reranked_results.items():\n",
    "                entry = {\n",
    "                    \"query_id\": qid,\n",
    "                    \"retrieved_documents\": docs\n",
    "                }\n",
    "                f.write(ujson.dumps(entry) + '\\n')\n",
    "\n",
    "        print(\"nDCG@10 (Model):\", ndcg.compute_average_ndcg(questions_file_path=QUESTIONS_FILE, results_file_path=validation_file, k=10))\n",
    "\n",
    "        # Track best performance, and save the model's state\n",
    "        if avg_vloss < best_vloss:\n",
    "            best_vloss = avg_vloss\n",
    "            model_path = '../output/model/model_{}_{}.pt'.format(timestamp, epoch + 1)\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    return model, model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e4217b",
   "metadata": {},
   "source": [
    "## Create Model\n",
    "\n",
    "The CNNInteractionBasedModel is a PyTorch neural network model designed to rerank documents based on their relevance to a given query. It leverages convolutional layers to capture interactions between query and document embeddings.\n",
    "\n",
    "**Key Components**:\n",
    "\n",
    "- **Embedding Layer**: Converts token IDs into dense vectors. Supports loading pretrained embeddings (e.g., GloVe).\n",
    "- **Convolutional Layer**: Captures local interactions between query and document embeddings.\n",
    "- **Activation Function**: Applies ReLU activation to introduce non-linearity.\n",
    "- **Pooling Layer**: Aggregates features using adaptive max pooling.\n",
    "- **Fully Connected Layer**: Maps extracted features to a single relevance score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8afff8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings + Model\n",
    "if os.path.exists(PRETRAINED_EMB):\n",
    "    print(\"Loading pretrained embeddings...\")\n",
    "    pretrained_embeddings = load_pretrained_embeddings(PRETRAINED_EMB, tokenizer, embedding_dim=300)\n",
    "else:\n",
    "    print(\"No pretrained embeddings found, using random init.\")\n",
    "    pretrained_embeddings = None\n",
    "\n",
    "print(\"Vocabulary size is: \", tokenizer.vocab_size)\n",
    "\n",
    "model = CNNInteractionBasedModel(vocab_size=tokenizer.vocab_size, pretrained_embeddings=pretrained_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4f5191",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "### Training Setup \n",
    "\n",
    "We configure the model training with the following hyperparameters:\n",
    "\n",
    "- **EPOCHS**: number of times that the model will see the entire training dataset.\n",
    "- **BATCH_SIZE**: training samples are processed in batches query–document pairs.\n",
    "- **LR**: learning rate controls how fast the optimizer updates the weights.\n",
    "\n",
    "The training will automatically use a **GPU (CUDA)** if available, otherwise it will fall back to the **CPU**.\n",
    "\n",
    "### Training the Model\n",
    "\n",
    "We call the `train_model` function, which:\n",
    "\n",
    "**1.** Loads the training dataset (queries, documents, labels).\n",
    "\n",
    "**2.** Prepares data batches with proper padding.\n",
    "\n",
    "**3.** Optimizes the model using **binary cross-entropy loss**, where:\n",
    "\n",
    "- **positive samples (label = 1)**: documents in the gold standard set for a query.\n",
    "- **negative samples (label = 0)**: retrieved documents not in the gold set.\n",
    "\n",
    "**4.** Runs the training loop for the defined number of epochs.\n",
    "\n",
    "**5.** Returns the trained model, ready for reranking/evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ab6c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "LR = 1e-3\n",
    "\n",
    "print(\"Training new model...\")\n",
    "model, model_path = train_model(model, epochs=EPOCHS, lr=LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d6f706",
   "metadata": {},
   "source": [
    "## Computing Ranking Metrics (BM25 Results) \n",
    "\n",
    "The system includes a script to compute the **Normalized Discounted Cumulative Gain (nDCG)** metric, which evaluates the quality of the ranked retrieval results. For this manner, execute the `nDCG.py` script.\n",
    "\n",
    "#### How nDCG Works\n",
    "\n",
    "- **DCG (Discounted Cumulative Gain)**: Measures the gain (relevance) of each document in the result list, discounted by its position in the list.\n",
    "- **IDCG (Ideal DCG)**: The maximum possible DCG achievable, obtained by an ideal ranking of documents.\n",
    "- **nDCG**: The ratio of DCG to IDCG, normalized to a value between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036068aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute nDCG for the given results\n",
    "ndcg.compute_average_ndcg(\n",
    "    questions_file_path=QUESTIONS_FILE,\n",
    "    results_file_path=BM25_OUPUT_FILE,\n",
    "    k=10\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366a0c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model\n",
    "model = CNNInteractionBasedModel(vocab_size=tokenizer.vocab_size)\n",
    "model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
    "model.to(DEVICE)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2062e7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset for reranking\n",
    "dataset = PointWiseDataset(QUESTIONS_FILE, BM25_FILE, CORPUS_FILE, tokenizer)\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=build_collate_fn(tokenizer, max_q_len, max_d_len),\n",
    "    pin_memory=(DEVICE.type == \"cuda\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0851d3",
   "metadata": {},
   "source": [
    "## Reranking with the Neural Model\n",
    "\n",
    "1. **Batch scoring**  \n",
    "- For each batch from the DataLoader, we take the tokenized queries and candidate documents.  \n",
    "- The model outputs a **relevance score** for each (query, document) pair.  \n",
    "\n",
    "2. **Collect scores per query**  \n",
    "- We store the `(document_id, score)` pairs for every query.  \n",
    "\n",
    "3. **Sort candidates**  \n",
    "- For each query, we sort the candidate documents in descending order of model score.  \n",
    "- This step produces the final reranked list of documents for each query.  \n",
    "\n",
    "4. **Save results**  \n",
    "- The results are saved in a JSONL file with the format:\n",
    "   ```json\n",
    "   {\n",
    "      \"query_id\": \"...\",\n",
    "      \"retrieved_documents\": [\"doc1\", \"doc2\", \"doc3\", ...]\n",
    "   }\n",
    "   ```\n",
    "- This keeps the same structure as the BM25 file, making it easy to compare baseline vs reranked performance.  \n",
    "\n",
    "This reranking step does not retrieve new documents — it only **reorders the BM25 shortlist** according to the learned neural model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa1166a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run reranking\n",
    "reranked_results = {}\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in loader:\n",
    "        q_tokens = batch[\"question_token_ids\"].to(DEVICE)\n",
    "        d_tokens = batch[\"document_token_ids\"].to(DEVICE)\n",
    "        qids = batch[\"query_ids\"]\n",
    "        dids = batch[\"document_ids\"]\n",
    "\n",
    "        scores = model(q_tokens, d_tokens)\n",
    "        scores = torch.sigmoid(scores).cpu().numpy()\n",
    "\n",
    "        for i, qid in enumerate(qids):\n",
    "            if qid not in reranked_results:\n",
    "                reranked_results[qid] = []\n",
    "            reranked_results[qid].append((dids[i], float(scores[i])))\n",
    "\n",
    "# sort by model score\n",
    "for qid, doc_scores in reranked_results.items():\n",
    "    reranked_results[qid] = [doc for doc, _ in sorted(doc_scores, key=lambda x: x[1], reverse=True)]\n",
    "\n",
    "output_file = os.path.join(OUTPUT_DIR, 'ranked_questions_model.jsonl')\n",
    "with open(output_file, 'w') as f:\n",
    "    for qid, docs in reranked_results.items():\n",
    "        entry = {\n",
    "            \"query_id\": qid,\n",
    "            \"retrieved_documents\": docs\n",
    "        }\n",
    "        f.write(ujson.dumps(entry) + '\\n')\n",
    "\n",
    "print(f\"Reranked results saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a07e1ab",
   "metadata": {},
   "source": [
    "## Evaluate Retrieved Documents (Model Reranking)\n",
    "\n",
    "Here we compute the **Normalized Discounted Cumulative Gain (nDCG)** metric, which evaluates the quality of the ranked retrieval results after model reranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc602d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Reranked nDCG@10 (Model)\")\n",
    "\n",
    "# Compute nDCG after reranking\n",
    "ndcg.compute_average_ndcg(\n",
    "    questions_file_path=QUESTIONS_FILE,\n",
    "    results_file_path=output_file,\n",
    "    k=10\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
