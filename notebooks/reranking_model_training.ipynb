{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb9a2f34",
   "metadata": {},
   "source": [
    "# Neural Reranker \n",
    "\n",
    "## Overview \n",
    "\n",
    "The Neural Reranker and Evaluation System enhances the baseline Information Retrieval (IR) system by introducing a CNN-based neural reranker model to improve the relevance of retrieved documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c54987b",
   "metadata": {},
   "source": [
    "## Imports & Config\n",
    "\n",
    "Here we specify all the necessary modules, the required files and paths:\n",
    "- **Pretrained Embeddings**: Path to the pretrained embeddings file (txt format).\n",
    "- **Corpus File**: Path to the corpus file (JSONL format).\n",
    "- **Questions File**: Path to the questions file (JSONL format).\n",
    "- **Questions Ranked (BM25)**: Path to the BM25-ranked file (JSONL format).\n",
    "- **Training Data**: Path to the questions training file (JSONL format). \n",
    "- **BM25 Ranked Questions Training Data**: Path to the BM25-ranked training file (JSONL format).\n",
    "- **Ouput File**: Path to save the reranked results (JSONL format).\n",
    "- **Model Checkpoint**: Path to the trained model checkpoint (train new model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51809637",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import ujson\n",
    "from datetime import datetime \n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Add parent directory to path to import from src\n",
    "sys.path.append('..')\n",
    "\n",
    "import src.evaluation as ndcg \n",
    "from src.model import BertCrossEncoder as Model\n",
    "from src.tokenizer import Tokenizer\n",
    "from src.dataset import PointWiseDataset\n",
    "from src.utils import (\n",
    "    compute_dynamic_max_len,\n",
    "    build_collate_fn, \n",
    "    get_all_doc_texts, \n",
    ")\n",
    "\n",
    "TOKENIZER_CONFIG = {\n",
    "    'min_token_length': 3,\n",
    "    'lowercase': True,\n",
    "    'stem': True,\n",
    "    'stopwords': None  # Can provide a set of stopwords\n",
    "}\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "NEGATIVES = 4\n",
    "OUTPUT_DIR = \"../output\"\n",
    "PRETRAINED_EMB = \"../data/glove.42B.300d.txt\"\n",
    "CORPUS_FILE = \"../data/MEDLINE_2024_Baseline.jsonl\"\n",
    "QUESTIONS_FILE = \"../data/questions.jsonl\"\n",
    "BM25_FILE = \"../data/questions_bm25_ranked.jsonl\"\n",
    "TRAIN_Q_FILE = \"../data/training_data.jsonl\"\n",
    "TRAIN_BM25_FILE = \"../data/training_data_bm25_ranked.jsonl\"\n",
    "BM25_OUPUT_FILE = \"../output/ranked_questions.jsonl\"\n",
    "OUTPUT_FILE = \"../output/final_ranked_questions.jsonl\"\n",
    "MODEL_CHECKPOINT = \"../output/model\"\n",
    "\n",
    "Path(MODEL_CHECKPOINT).mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Output model directory ready: {MODEL_CHECKPOINT}\")\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7489b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer \n",
    "\n",
    "print(\"Tokenizer configuration:\")\n",
    "for key, value in TOKENIZER_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "tokenizer = Tokenizer(\n",
    "    TOKENIZER_CONFIG.get('min_token_length', 3),\n",
    "    TOKENIZER_CONFIG.get('lowercase', True),\n",
    "    TOKENIZER_CONFIG.get('stem', False),\n",
    "    set(TOKENIZER_CONFIG.get('stopwords', [])) if TOKENIZER_CONFIG.get('stopwords') else None\n",
    ")\n",
    "\n",
    "text = get_all_doc_texts(CORPUS_FILE)\n",
    "tokenizer.fit(text)\n",
    "\n",
    "# Get all tokens tokenizer learned\n",
    "corpus_tokens = list(tokenizer.token_to_id.keys())\n",
    "print(f\"Number of tokens in tokenizer: {len(corpus_tokens)}\")\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Only add tokens that BERT doesn't already have\n",
    "vocab_set = set(bert_tokenizer.get_vocab().keys())\n",
    "new_tokens = [t for t in corpus_tokens if t not in vocab_set]\n",
    "print(f\"Adding {len(new_tokens)} new tokens to BERT tokenizer\")\n",
    "\n",
    "bert_tokenizer.add_tokens(new_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606b0b9f",
   "metadata": {},
   "source": [
    "## Dataset and DataLoader\n",
    "The `PointWiseDataset` and `DataLoader` classes encapsulate the process of pulling your data from storage and exposing it to your training loop in batches.\n",
    "\n",
    "The `Dataset` is responsible for accessing and processing single instances of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d829f639",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = compute_dynamic_max_len(\n",
    "    questions_file=QUESTIONS_FILE,\n",
    "    bm25_ranked_file=BM25_FILE,\n",
    "    corpus_file=CORPUS_FILE,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"Max length: \", max_len)\n",
    "\n",
    "print(\"Loading training dataset...\")\n",
    "train_dataset = PointWiseDataset(TRAIN_Q_FILE, TRAIN_BM25_FILE, CORPUS_FILE, bert_tokenizer, mode=\"train\", negative_ratio=NEGATIVES, include_random_negatives=False, max_seq_len=max_len)\n",
    "\n",
    "print(\"Loading validation dataset...\")\n",
    "validation_dataset = PointWiseDataset(QUESTIONS_FILE, BM25_FILE, CORPUS_FILE, bert_tokenizer, mode=\"valid\", max_seq_len=max_len)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=build_collate_fn(),\n",
    "    shuffle=True,\n",
    "    pin_memory=(DEVICE.type == 'cuda')\n",
    ")\n",
    "\n",
    "validation_loader = DataLoader(\n",
    "    validation_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=build_collate_fn(),\n",
    "    pin_memory=(DEVICE.type == \"cuda\")\n",
    ")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0e44ab",
   "metadata": {},
   "source": [
    "## Training Procedure\n",
    "\n",
    "We train the reranking model in a pointwise classification setup, where the model learns to predict whether a retrieved document is **relevant (positive)** or **not relevant (negative)** to a given query.\n",
    "\n",
    "**1. Input Data**\n",
    "- Each **query** comes with a **set of retrieved documents** (from BM25).\n",
    "- Each (query, document) pair is labeled:\n",
    "    - **Positive (label=1)**: if the document is listed in the **gold-standard set** for that query.\n",
    "    - **Negative (label=0)**: otherwise.\n",
    "\n",
    "This produces training samples of the form:\n",
    "\n",
    "```sh\n",
    "(query_text, document_text, label)\n",
    "```\n",
    "\n",
    "**2. Negative/Positive Ratio**\n",
    "\n",
    "- Since BM25 retrieves many more non-relevant documents than relevant ones, the dataset is **highly imbalanced**.\n",
    "- To counter this, we use a **negative sampling strategy**:\n",
    "- For every **positive document**, we sample up to **k negatives** (e.g., 2× more negatives).\n",
    "    - This yields an approximate **1:2 ratio** of positives to negatives.\n",
    "    - This prevents the model from being biased toward always predicting \"non-relevant\".\n",
    "\n",
    "**3. Negative Mining Approach**\n",
    "\n",
    "We start with **BM25 retrieval** as a source of candidate documents:\n",
    "\n",
    "- Positives are guaranteed to be included (if retrieved).\n",
    "- Negatives are chosen from the **top BM25 results that are not in the gold standard**.\n",
    "    - These negatives are **“hard negatives”**, because they were highly ranked by BM25 but are not relevant.\n",
    "    - Training on these improves discrimination compared to random negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e73716",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, epochs=1, lr=1e-3):\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    # Unfreeze new embeddings\n",
    "    for param in model.bert.get_input_embeddings().parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # unfreeze last 2 encoder layers\n",
    "    for layer in model.bert.encoder.layer[-2:]:\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=lr\n",
    "    )\n",
    "\n",
    "    loss_fn = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([float(NEGATIVES)], device=DEVICE))\n",
    "    best_vloss = 1_000_000.\n",
    "    model = model.to(DEVICE)\n",
    "    model_path = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print('EPOCH {}:'.format(epoch + 1))\n",
    "\n",
    "        for name, param in model.named_parameters():\n",
    "            if \"encoder.layer.10\" in name or \"encoder.layer.11\" in name:\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "\n",
    "        model.train(True)\n",
    "        running_loss = 0.\n",
    "        last_loss = 0.\n",
    "\n",
    "        for i, data in enumerate(train_loader):\n",
    "            input_tokens_ids = data[\"input_ids\"].to(DEVICE, non_blocking=True)\n",
    "            attention_mask = data[\"attention_mask\"].to(DEVICE, non_blocking=True)\n",
    "            labels = data[\"label\"].to(DEVICE, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            scores = model(input_tokens_ids, attention_mask)\n",
    "            loss = loss_fn(scores, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if i % 1000 == 999:\n",
    "                last_loss = running_loss / 1000\n",
    "                print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "                running_loss = 0.\n",
    "\n",
    "        running_vloss = 0.0\n",
    "        # Set the model to evaluation mode, disabling dropout and using population\n",
    "        # statistics for batch normalization.\n",
    "        model.eval()\n",
    "\n",
    "        reranked_results = {}\n",
    "\n",
    "        # Disable gradient computation and reduce memory consumption.\n",
    "        with torch.no_grad():\n",
    "            for vdata in validation_loader:\n",
    "                val_input_tokens_ids = vdata[\"input_ids\"].to(DEVICE, non_blocking=True)\n",
    "                val_attention_mask = vdata[\"attention_mask\"].to(DEVICE, non_blocking=True)\n",
    "                labels = vdata[\"label\"].to(DEVICE, non_blocking=True)\n",
    "                qids = vdata[\"query_ids\"]\n",
    "                dids = vdata[\"document_ids\"]\n",
    "\n",
    "                voutputs = model(val_input_tokens_ids, val_attention_mask)\n",
    "                vloss = loss_fn(voutputs, labels)\n",
    "                running_vloss += vloss.item()\n",
    "\n",
    "                scores = torch.sigmoid(voutputs).cpu().numpy()\n",
    "                for x, qid in enumerate(qids):\n",
    "                    if qid not in reranked_results:\n",
    "                        reranked_results[qid] = []\n",
    "                    reranked_results[qid].append((dids[x], float(scores[x])))\n",
    "\n",
    "        avg_vloss = running_vloss / (i + 1)\n",
    "        print('LOSS train {} valid {}'.format(last_loss, avg_vloss))\n",
    "\n",
    "        # sort by model score\n",
    "        for qid, doc_scores in reranked_results.items():\n",
    "            reranked_results[qid] = [doc for doc, _ in sorted(doc_scores, key=lambda x: x[1], reverse=True)]\n",
    "\n",
    "        validation_file = os.path.join(OUTPUT_DIR, 'validation_ranked_questions_model.jsonl')\n",
    "        with open(validation_file, 'w') as f:\n",
    "            for qid, docs in reranked_results.items():\n",
    "                entry = {\n",
    "                    \"query_id\": qid,\n",
    "                    \"retrieved_documents\": docs\n",
    "                }\n",
    "                f.write(ujson.dumps(entry) + '\\n')\n",
    "\n",
    "        print(\"nDCG@10 (Model):\", ndcg.compute_average_ndcg(questions_file_path=QUESTIONS_FILE, results_file_path=validation_file, k=10, print_out=False))\n",
    "\n",
    "        # Track best performance, and save the model's state\n",
    "        if avg_vloss < best_vloss:\n",
    "            best_vloss = avg_vloss\n",
    "            model_path = '../output/model/model_{}_{}.pt'.format(timestamp, epoch + 1)\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    return model, model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e4217b",
   "metadata": {},
   "source": [
    "## Create Model\n",
    "\n",
    "The CNNInteractionBasedModel is a PyTorch neural network model designed to rerank documents based on their relevance to a given query. It leverages convolutional layers to capture interactions between query and document embeddings.\n",
    "\n",
    "**Key Components**:\n",
    "\n",
    "- **Embedding Layer**: Converts token IDs into dense vectors. Supports loading pretrained embeddings (e.g., GloVe).\n",
    "- **Convolutional Layer**: Captures local interactions between query and document embeddings.\n",
    "- **Activation Function**: Applies ReLU activation to introduce non-linearity.\n",
    "- **Pooling Layer**: Aggregates features using adaptive max pooling.\n",
    "- **Fully Connected Layer**: Maps extracted features to a single relevance score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8afff8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings + Model\n",
    "print(\"Vocabulary size is: \", bert_tokenizer.vocab_size)\n",
    "\n",
    "model = Model(vocab_size=len(bert_tokenizer))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4f5191",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "### Training Setup \n",
    "\n",
    "We configure the model training with the following hyperparameters:\n",
    "\n",
    "- **EPOCHS**: number of times that the model will see the entire training dataset.\n",
    "- **BATCH_SIZE**: training samples are processed in batches query–document pairs.\n",
    "- **LR**: learning rate controls how fast the optimizer updates the weights.\n",
    "\n",
    "The training will automatically use a **GPU (CUDA)** if available, otherwise it will fall back to the **CPU**.\n",
    "\n",
    "### Training the Model\n",
    "\n",
    "We call the `train_model` function, which:\n",
    "\n",
    "**1.** Loads the training dataset (queries, documents, labels).\n",
    "\n",
    "**2.** Prepares data batches with proper padding.\n",
    "\n",
    "**3.** Optimizes the model using **binary cross-entropy loss**, where:\n",
    "\n",
    "- **positive samples (label = 1)**: documents in the gold standard set for a query.\n",
    "- **negative samples (label = 0)**: retrieved documents not in the gold set.\n",
    "\n",
    "**4.** Runs the training loop for the defined number of epochs.\n",
    "\n",
    "**5.** Returns the trained model, ready for reranking/evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ab6c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "LR = 2e-5\n",
    "\n",
    "print(\"Training new model...\")\n",
    "model, model_path = train_model(model, epochs=EPOCHS, lr=LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d6f706",
   "metadata": {},
   "source": [
    "## Computing Ranking Metrics (BM25 Results) \n",
    "\n",
    "The system includes a script to compute the **Normalized Discounted Cumulative Gain (nDCG)** metric, which evaluates the quality of the ranked retrieval results. For this manner, execute the `nDCG.py` script.\n",
    "\n",
    "#### How nDCG Works\n",
    "\n",
    "- **DCG (Discounted Cumulative Gain)**: Measures the gain (relevance) of each document in the result list, discounted by its position in the list.\n",
    "- **IDCG (Ideal DCG)**: The maximum possible DCG achievable, obtained by an ideal ranking of documents.\n",
    "- **nDCG**: The ratio of DCG to IDCG, normalized to a value between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036068aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute nDCG for the given results\n",
    "ndcg.compute_average_ndcg(\n",
    "    questions_file_path=QUESTIONS_FILE,\n",
    "    results_file_path=BM25_OUPUT_FILE,\n",
    "    k=10\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366a0c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model\n",
    "model = Model(vocab_size=tokenizer.vocab_size)\n",
    "model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
    "model.to(DEVICE)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2062e7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset for reranking\n",
    "dataset = PointWiseDataset(QUESTIONS_FILE, BM25_FILE, CORPUS_FILE, tokenizer, max_seq_len=max_len)\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=build_collate_fn(),\n",
    "    pin_memory=(DEVICE.type == \"cuda\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0851d3",
   "metadata": {},
   "source": [
    "## Reranking with the Neural Model\n",
    "\n",
    "1. **Batch scoring**  \n",
    "- For each batch from the DataLoader, we take the tokenized queries and candidate documents.  \n",
    "- The model outputs a **relevance score** for each (query, document) pair.  \n",
    "\n",
    "2. **Collect scores per query**  \n",
    "- We store the `(document_id, score)` pairs for every query.  \n",
    "\n",
    "3. **Sort candidates**  \n",
    "- For each query, we sort the candidate documents in descending order of model score.  \n",
    "- This step produces the final reranked list of documents for each query.  \n",
    "\n",
    "4. **Save results**  \n",
    "- The results are saved in a JSONL file with the format:\n",
    "   ```json\n",
    "   {\n",
    "      \"query_id\": \"...\",\n",
    "      \"retrieved_documents\": [\"doc1\", \"doc2\", \"doc3\", ...]\n",
    "   }\n",
    "   ```\n",
    "- This keeps the same structure as the BM25 file, making it easy to compare baseline vs reranked performance.  \n",
    "\n",
    "This reranking step does not retrieve new documents — it only **reorders the BM25 shortlist** according to the learned neural model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa1166a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run reranking\n",
    "reranked_results = {}\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in loader:\n",
    "        input_tokens_ids = batch[\"input_ids\"].to(DEVICE, non_blocking=True)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE, non_blocking=True)\n",
    "        qids = batch[\"query_ids\"]\n",
    "        dids = batch[\"document_ids\"]\n",
    "\n",
    "        scores = model(input_tokens_ids, attention_mask)\n",
    "        scores = torch.sigmoid(scores).cpu().numpy()\n",
    "\n",
    "        for i, qid in enumerate(qids):\n",
    "            if qid not in reranked_results:\n",
    "                reranked_results[qid] = []\n",
    "            reranked_results[qid].append((dids[i], float(scores[i])))\n",
    "\n",
    "# sort by model score\n",
    "for qid, doc_scores in reranked_results.items():\n",
    "    reranked_results[qid] = [doc for doc, _ in sorted(doc_scores, key=lambda x: x[1], reverse=True)]\n",
    "\n",
    "output_file = os.path.join(OUTPUT_DIR, 'ranked_questions_model.jsonl')\n",
    "with open(output_file, 'w') as f:\n",
    "    for qid, docs in reranked_results.items():\n",
    "        entry = {\n",
    "            \"query_id\": qid,\n",
    "            \"retrieved_documents\": docs\n",
    "        }\n",
    "        f.write(ujson.dumps(entry) + '\\n')\n",
    "\n",
    "print(f\"Reranked results saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a07e1ab",
   "metadata": {},
   "source": [
    "## Evaluate Retrieved Documents (Model Reranking)\n",
    "\n",
    "Here we compute the **Normalized Discounted Cumulative Gain (nDCG)** metric, which evaluates the quality of the ranked retrieval results after model reranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc602d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Reranked nDCG@10 (Model)\")\n",
    "\n",
    "# Compute nDCG after reranking\n",
    "ndcg.compute_average_ndcg(\n",
    "    questions_file_path=QUESTIONS_FILE,\n",
    "    results_file_path=output_file,\n",
    "    k=10\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
