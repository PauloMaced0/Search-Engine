{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb9a2f34",
   "metadata": {},
   "source": [
    "# Neural Reranker \n",
    "\n",
    "## Overview \n",
    "\n",
    "The Neural Reranker and Evaluation System enhances the baseline Information Retrieval (IR) system by introducing a CNN-based neural reranker model to improve the relevance of retrieved documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c54987b",
   "metadata": {},
   "source": [
    "## Imports & Config\n",
    "\n",
    "Here we specify all the necessary modules, the required files and paths:\n",
    "- **Pretrained Embeddings**: Path to the pretrained embeddings file (txt format).\n",
    "- **Corpus File**: Path to the corpus file (JSONL format).\n",
    "- **Questions File**: Path to the questions file (JSONL format).\n",
    "- **Questions Ranked (BM25)**: Path to the BM25-ranked file (JSONL format).\n",
    "- **Training Data**: Path to the questions training file (JSONL format). \n",
    "- **BM25 Ranked Questions Training Data**: Path to the BM25-ranked training file (JSONL format).\n",
    "- **Ouput File**: Path to save the reranked results (JSONL format).\n",
    "- **Model Checkpoint**: Path to the trained model checkpoint (train new model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51809637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output model directory ready: ../output/model\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datetime import datetime \n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Add parent directory to path to import from src\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.model import CNNInteractionBasedModel, Tokenizer, PointWiseDataset\n",
    "from src.utils import (\n",
    "    load_pretrained_embeddings, \n",
    "    build_collate_fn, \n",
    "    get_all_doc_texts, \n",
    "    get_questions, \n",
    ")\n",
    "\n",
    "PRETRAINED_EMB = \"../data/glove.42B.300d.txt\"\n",
    "CORPUS_FILE = \"../data/MEDLINE_2024_Baseline.jsonl\"\n",
    "QUESTIONS_FILE = \"../data/questions.jsonl\"\n",
    "BM25_FILE = \"../data/questions_bm25_ranked.jsonl\"\n",
    "TRAIN_Q_FILE = \"../data/training_data.jsonl\"\n",
    "TRAIN_BM25_FILE = \"../data/training_data_bm25_ranked.jsonl\"\n",
    "OUTPUT_FILE = \"../output/final_ranked_questions.jsonl\"\n",
    "MODEL_CHECKPOINT = \"../output/model\"\n",
    "\n",
    "Path(MODEL_CHECKPOINT).mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Output model directory ready: {MODEL_CHECKPOINT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0e44ab",
   "metadata": {},
   "source": [
    "## Training Procedure\n",
    "\n",
    "We train the reranking model in a pointwise classification setup, where the model learns to predict whether a retrieved document is **relevant (positive)** or **not relevant (negative)** to a given query.\n",
    "\n",
    "**1. Input Data**\n",
    "- Each **query** comes with a **set of retrieved documents** (from BM25).\n",
    "- Each (query, document) pair is labeled:\n",
    "    - **Positive (label=1)**: if the document is listed in the **gold-standard set** for that query.\n",
    "    - **Negative (label=0)**: otherwise.\n",
    "\n",
    "This produces training samples of the form:\n",
    "\n",
    "```sh\n",
    "(query_text, document_text, label)\n",
    "```\n",
    "\n",
    "**2. Negative/Positive Ratio**\n",
    "\n",
    "- Since BM25 retrieves many more non-relevant documents than relevant ones, the dataset is **highly imbalanced**.\n",
    "- To counteract this, we use a **negative sampling strategy**:\n",
    "- For every **positive document**, we sample up to **k negatives** (e.g., 2× more negatives).\n",
    "    - This yields an approximate **1:2 ratio** of positives to negatives.\n",
    "    - This prevents the model from being biased toward always predicting \"non-relevant\".\n",
    "\n",
    "**3. Negative Mining Approach**\n",
    "\n",
    "We start with **BM25 retrieval** as a source of candidate documents:\n",
    "\n",
    "- Positives are guaranteed to be included (if retrieved).\n",
    "- Negatives are chosen from the **top BM25 results that are not in the gold standard**.\n",
    "    - These negatives are **“hard negatives”**, because they were highly ranked by BM25 but are not relevant.\n",
    "    - Training on these improves discrimination compared to random negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0e73716",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, tokenizer, training_questions_file, training_ranked_file, corpus_file, device, batch_size=64, epochs=5, lr=1e-3):\n",
    "    print(\"Loading training dataset...\")\n",
    "\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "    train_dataset = PointWiseDataset(training_questions_file, training_ranked_file, corpus_file, tokenizer, return_label=True)\n",
    "\n",
    "    # Determine max lengths\n",
    "    q_lens = [len(train_dataset[i][\"question_token_ids\"]) for i in range(len(train_dataset))]\n",
    "    d_lens = [len(train_dataset[i][\"document_token_ids\"]) for i in range(len(train_dataset))]\n",
    "    max_q_len = max(3, max(q_lens))\n",
    "    max_d_len = max(3, max(d_lens))\n",
    "    print(f\"Max Q length: {max_q_len}, Max D length: {max_d_len}\")\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=build_collate_fn(tokenizer, max_q_len, max_d_len),\n",
    "        shuffle=True,\n",
    "        pin_memory=(device.type == 'cuda')\n",
    "    )\n",
    "\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_vloss = 1_000_000.\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print('EPOCH {}:'.format(epoch + 1))\n",
    "        model.train(True)\n",
    "\n",
    "        running_loss = 0.\n",
    "        last_loss = 0.\n",
    "\n",
    "        for i, data in enumerate(train_loader):\n",
    "            qids = data[\"question_token_ids\"]\n",
    "            dids = data[\"document_token_ids\"]\n",
    "            labels = data[\"label\"]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            scores = model(qids, dids)\n",
    "            loss = loss_fn(scores, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if i % 1000 == 999:\n",
    "                last_loss = running_loss / 1000\n",
    "                print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "                running_loss = 0.\n",
    "\n",
    "        running_vloss = 0.0\n",
    "        # Set the model to evaluation mode, disabling dropout and using population\n",
    "        # statistics for batch normalization.\n",
    "        model.eval()\n",
    "\n",
    "        # Disable gradient computation and reduce memory consumption.\n",
    "        with torch.no_grad():\n",
    "            for i, vdata in enumerate(train_loader): # validation loader is the same as the train loader\n",
    "                qids = vdata[\"question_token_ids\"]\n",
    "                dids = vdata[\"document_token_ids\"]\n",
    "                labels = vdata[\"label\"]\n",
    "\n",
    "                voutputs = model(qids, dids)\n",
    "                vloss = loss_fn(voutputs, labels)\n",
    "                running_vloss += vloss.item()\n",
    "\n",
    "        avg_vloss = running_vloss / (i + 1)\n",
    "        print('LOSS train {} valid {}'.format(last_loss, avg_vloss))\n",
    "\n",
    "        # Track best performance, and save the model's state\n",
    "        if avg_vloss < best_vloss:\n",
    "            best_vloss = avg_vloss\n",
    "            model_path = '../output/model/model_{}_{}.pt'.format(timestamp, epoch + 1)\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e4217b",
   "metadata": {},
   "source": [
    "## Create Model\n",
    "\n",
    "The CNNInteractionBasedModel is a PyTorch neural network model designed to rerank documents based on their relevance to a given query. It leverages convolutional layers to capture interactions between query and document embeddings.\n",
    "\n",
    "**Key Components**:\n",
    "\n",
    "- **Embedding Layer**: Converts token IDs into dense vectors. Supports loading pretrained embeddings (e.g., GloVe).\n",
    "- **Convolutional Layer**: Captures local interactions between query and document embeddings.\n",
    "- **Activation Function**: Applies ReLU activation to introduce non-linearity.\n",
    "- **Pooling Layer**: Aggregates features using adaptive max pooling.\n",
    "- **Fully Connected Layer**: Maps extracted features to a single relevance score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8afff8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained embeddings...\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Tokenizer + Embeddings + Model\n",
    "tokenizer = Tokenizer()\n",
    "questions = get_questions(TRAIN_Q_FILE)\n",
    "documents = get_all_doc_texts(TRAIN_Q_FILE, TRAIN_BM25_FILE, CORPUS_FILE)\n",
    "tokenizer.fit(questions + documents)\n",
    "\n",
    "if os.path.exists(PRETRAINED_EMB):\n",
    "    print(\"Loading pretrained embeddings...\")\n",
    "    pretrained_embeddings = load_pretrained_embeddings(PRETRAINED_EMB, tokenizer, embedding_dim=300)\n",
    "else:\n",
    "    print(\"No pretrained embeddings found, using random init.\")\n",
    "    pretrained_embeddings = None\n",
    "\n",
    "print(\"Vocabulary size is: \", tokenizer.vocab_size)\n",
    "\n",
    "model = CNNInteractionBasedModel(vocab_size=tokenizer.vocab_size, pretrained_embeddings=pretrained_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4f5191",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "### Training Setup \n",
    "\n",
    "We configure the model training with the following hyperparameters:\n",
    "\n",
    "- **EPOCHS**: number of times that the model will see the entire training dataset.\n",
    "- **BATCH_SIZE**: training samples are processed in batches query–document pairs.\n",
    "- **LR**: learning rate controls how fast the optimizer updates the weights.\n",
    "\n",
    "The training will automatically use a **GPU (CUDA)** if available, otherwise it will fall back to the **CPU**.\n",
    "\n",
    "### Training the Model\n",
    "\n",
    "We call the `train_model` function, which:\n",
    "\n",
    "**1.** Loads the training dataset (queries, documents, labels).\n",
    "\n",
    "**2.** Prepares data batches with proper padding.\n",
    "\n",
    "**3.** Optimizes the model using **binary cross-entropy loss**, where:\n",
    "\n",
    "- **positive samples (label = 1)**: documents in the gold standard set for a query.\n",
    "- **negative samples (label = 0)**: retrieved documents not in the gold set.\n",
    "\n",
    "**4.** Runs the training loop for the defined number of epochs.\n",
    "\n",
    "**5.** Returns the trained model, ready for reranking/evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d2ab6c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Training new model...\n",
      "Loading training dataset...\n",
      "Max Q length: 30, Max D length: 1785\n",
      "EPOCH 1:\n",
      "  batch 1000 loss: 0.13784657931514085\n",
      "  batch 2000 loss: 0.13973148022405804\n",
      "  batch 3000 loss: 0.1431128241531551\n",
      "  batch 4000 loss: 0.14808657264336944\n",
      "  batch 5000 loss: 0.14313516185060143\n",
      "  batch 6000 loss: 0.14648264915309847\n",
      "  batch 7000 loss: 0.1511464210636914\n",
      "LOSS train 0.1511464210636914 valid 0.0958528913795787\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 1\n",
    "BATCH_SIZE = 64\n",
    "LR = 1e-3\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "print(\"Training new model...\")\n",
    "model = train_model(\n",
    "    model, tokenizer, TRAIN_Q_FILE, TRAIN_BM25_FILE,\n",
    "    CORPUS_FILE, device, batch_size=BATCH_SIZE, epochs=EPOCHS, lr=LR\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
