{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb9a2f34",
   "metadata": {},
   "source": [
    "# Neural Reranker \n",
    "\n",
    "## Overview \n",
    "\n",
    "The Neural Reranker and Evaluation System enhances the baseline Information Retrieval (IR) system by introducing a CNN-based neural reranker model to improve the relevance of retrieved documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c54987b",
   "metadata": {},
   "source": [
    "## Imports & Config\n",
    "\n",
    "Here we specify all the necessary modules, the required files and paths:\n",
    "- **Pretrained Embeddings**: Path to the pretrained embeddings file (txt format).\n",
    "- **Corpus File**: Path to the corpus file (JSONL format).\n",
    "- **Questions File**: Path to the questions file (JSONL format).\n",
    "- **Questions Ranked (BM25)**: Path to the BM25-ranked file (JSONL format).\n",
    "- **Training Data**: Path to the questions training file (JSONL format). \n",
    "- **BM25 Ranked Questions Training Data**: Path to the BM25-ranked training file (JSONL format).\n",
    "- **Ouput File**: Path to save the reranked results (JSONL format).\n",
    "- **Model Checkpoint**: Path to the trained model checkpoint (train new model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51809637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output model directory ready: ../output/model\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import ujson\n",
    "from datetime import datetime \n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Add parent directory to path to import from src\n",
    "sys.path.append('..')\n",
    "\n",
    "import src.evaluation as ndcg \n",
    "from src.model import BertCrossEncoder as Model\n",
    "from src.tokenizer import Tokenizer\n",
    "from src.dataset import PointWiseDataset\n",
    "from src.utils import (\n",
    "    build_collate_fn, \n",
    "    # get_all_doc_texts, \n",
    ")\n",
    "\n",
    "TOKENIZER_CONFIG = {\n",
    "    'min_token_length': 3,\n",
    "    'lowercase': True,\n",
    "    'stem': True,\n",
    "    'stopwords': None  # Can provide a set of stopwords\n",
    "}\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "NEGATIVES = 4\n",
    "OUTPUT_DIR = \"../output\"\n",
    "PRETRAINED_EMB = \"../data/glove.42B.300d.txt\"\n",
    "CORPUS_FILE = \"../data/MEDLINE_2024_Baseline.jsonl\"\n",
    "QUESTIONS_FILE = \"../data/questions.jsonl\"\n",
    "BM25_FILE = \"../data/questions_bm25_ranked.jsonl\"\n",
    "TRAIN_Q_FILE = \"../data/training_data.jsonl\"\n",
    "TRAIN_BM25_FILE = \"../data/training_data_bm25_ranked.jsonl\"\n",
    "BM25_OUPUT_FILE = \"../output/ranked_questions.jsonl\"\n",
    "OUTPUT_FILE = \"../output/final_ranked_questions.jsonl\"\n",
    "MODEL_CHECKPOINT = \"../output/model\"\n",
    "\n",
    "Path(MODEL_CHECKPOINT).mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Output model directory ready: {MODEL_CHECKPOINT}\")\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b7489b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer \n",
    "\n",
    "# print(\"Tokenizer configuration:\")\n",
    "# for key, value in TOKENIZER_CONFIG.items():\n",
    "#     print(f\"  {key}: {value}\")\n",
    "\n",
    "# tokenizer = Tokenizer(\n",
    "#     TOKENIZER_CONFIG.get('min_token_length', 3),\n",
    "#     TOKENIZER_CONFIG.get('lowercase', True),\n",
    "#     TOKENIZER_CONFIG.get('stem', False),\n",
    "#     set(TOKENIZER_CONFIG.get('stopwords', [])) if TOKENIZER_CONFIG.get('stopwords') else None\n",
    "# )\n",
    "\n",
    "# text = get_all_doc_texts(CORPUS_FILE)\n",
    "# tokenizer.fit(text)\n",
    "\n",
    "# Get all tokens tokenizer learned\n",
    "# corpus_tokens = list(tokenizer.token_to_id.keys())\n",
    "# print(f\"Number of tokens in tokenizer: {len(corpus_tokens)}\")\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# # Only add tokens that BERT doesn't already have\n",
    "# vocab_set = set(bert_tokenizer.get_vocab().keys())\n",
    "# new_tokens = [t for t in corpus_tokens if t not in vocab_set]\n",
    "# print(f\"Adding {len(new_tokens)} new tokens to BERT tokenizer\")\n",
    "\n",
    "# bert_tokenizer.add_tokens(new_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606b0b9f",
   "metadata": {},
   "source": [
    "## Dataset and DataLoader\n",
    "The `PointWiseDataset` and `DataLoader` classes encapsulate the process of pulling your data from storage and exposing it to your training loop in batches.\n",
    "\n",
    "The `Dataset` is responsible for accessing and processing single instances of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d829f639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training dataset...\n",
      "Loading validation dataset...\n",
      "Training dataset size: 131,855\n",
      "Validation dataset size: 9,537\n",
      "Batches per epoch: 2,061\n",
      "Estimated time per epoch: ~6.9 minutes\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading training dataset...\")\n",
    "train_dataset = PointWiseDataset(TRAIN_Q_FILE, TRAIN_BM25_FILE, CORPUS_FILE, bert_tokenizer, mode=\"train\", negative_ratio=NEGATIVES, include_random_negatives=False)\n",
    "\n",
    "print(\"Loading validation dataset...\")\n",
    "validation_dataset = PointWiseDataset(QUESTIONS_FILE, BM25_FILE, CORPUS_FILE, bert_tokenizer, mode=\"valid\")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=build_collate_fn(),\n",
    "    shuffle=True,\n",
    "    pin_memory=(DEVICE.type == 'cuda')\n",
    ")\n",
    "\n",
    "validation_loader = DataLoader(\n",
    "    validation_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=build_collate_fn(),\n",
    "    pin_memory=(DEVICE.type == \"cuda\")\n",
    ")   \n",
    "\n",
    "print(f\"Training dataset size: {len(train_dataset):,}\")\n",
    "print(f\"Validation dataset size: {len(validation_dataset):,}\")\n",
    "print(f\"Batches per epoch: {len(train_loader):,}\")\n",
    "\n",
    "# Quick estimate:\n",
    "batches_per_epoch = len(train_loader)\n",
    "estimated_minutes = batches_per_epoch * 0.2 / 60  # Assuming ~0.2s per batch\n",
    "print(f\"Estimated time per epoch: ~{estimated_minutes:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0e44ab",
   "metadata": {},
   "source": [
    "## Training Procedure\n",
    "\n",
    "We train the reranking model in a pointwise classification setup, where the model learns to predict whether a retrieved document is **relevant (positive)** or **not relevant (negative)** to a given query.\n",
    "\n",
    "**1. Input Data**\n",
    "- Each **query** comes with a **set of retrieved documents** (from BM25).\n",
    "- Each (query, document) pair is labeled:\n",
    "    - **Positive (label=1)**: if the document is listed in the **gold-standard set** for that query.\n",
    "    - **Negative (label=0)**: otherwise.\n",
    "\n",
    "This produces training samples of the form:\n",
    "\n",
    "```sh\n",
    "(query_text, document_text, label)\n",
    "```\n",
    "\n",
    "**2. Negative/Positive Ratio**\n",
    "\n",
    "- Since BM25 retrieves many more non-relevant documents than relevant ones, the dataset is **highly imbalanced**.\n",
    "- To counter this, we use a **negative sampling strategy**:\n",
    "- For every **positive document**, we sample up to **k negatives** (e.g., 2× more negatives).\n",
    "    - This yields an approximate **1:2 ratio** of positives to negatives.\n",
    "    - This prevents the model from being biased toward always predicting \"non-relevant\".\n",
    "\n",
    "**3. Negative Mining Approach**\n",
    "\n",
    "We start with **BM25 retrieval** as a source of candidate documents:\n",
    "\n",
    "- Positives are guaranteed to be included (if retrieved).\n",
    "- Negatives are chosen from the **top BM25 results that are not in the gold standard**.\n",
    "    - These negatives are **“hard negatives”**, because they were highly ranked by BM25 but are not relevant.\n",
    "    - Training on these improves discrimination compared to random negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e73716",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, epochs=1, lr=1e-3):\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "    # Move model to device first\n",
    "    model = model.to(DEVICE)\n",
    "    \n",
    "    # Unfreeze last 2 encoder layers + classifier\n",
    "    # Freeze all BERT layers first\n",
    "    for param in model.bert.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Unfreeze last 2 encoder layers\n",
    "    for layer in model.bert.encoder.layer[-2:]:\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    # Always keep classifier trainable\n",
    "    for param in model.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    # Use different learning rates for BERT and classifier (discriminative fine-tuning)\n",
    "    optimizer = torch.optim.AdamW([\n",
    "        {'params': model.bert.encoder.layer[-2:].parameters(), 'lr': lr},\n",
    "        {'params': model.classifier.parameters(), 'lr': lr * 10}  # Higher LR for classifier\n",
    "    ], weight_decay=0.01)  # Add weight decay for regularization\n",
    "    \n",
    "    # Learning rate scheduler (recommended for BERT)\n",
    "    total_steps = len(train_loader) * epochs\n",
    "    scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "        optimizer, \n",
    "        start_factor=1.0, \n",
    "        end_factor=0.0, \n",
    "        total_iters=total_steps\n",
    "    )\n",
    "\n",
    "    loss_fn = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([float(NEGATIVES)], device=DEVICE))\n",
    "    best_vloss = 1_000_000.\n",
    "    model_path = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print('EPOCH {}:'.format(epoch + 1))\n",
    "\n",
    "        model.train()\n",
    "        running_loss = 0.\n",
    "        last_loss = 0.\n",
    "\n",
    "        for i, data in enumerate(train_loader):\n",
    "            input_tokens_ids = data[\"input_token_ids\"].to(DEVICE, non_blocking=True)\n",
    "            attention_mask = data[\"attention_mask\"].to(DEVICE, non_blocking=True)\n",
    "            labels = data[\"label\"].to(DEVICE, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            scores = model(input_tokens_ids, attention_mask)\n",
    "            loss = loss_fn(scores, labels)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if i % 100 == 99:\n",
    "                last_loss = running_loss / 100\n",
    "                print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "                running_loss = 0.\n",
    "\n",
    "        running_vloss = 0.0\n",
    "        # Set the model to evaluation mode, disabling dropout and using population\n",
    "        # statistics for batch normalization.\n",
    "        model.eval()\n",
    "\n",
    "        reranked_results = {}\n",
    "\n",
    "        # Disable gradient computation and reduce memory consumption.\n",
    "        with torch.no_grad():\n",
    "            for k, vdata in enumerate(validation_loader):\n",
    "                val_input_tokens_ids = vdata[\"input_token_ids\"].to(DEVICE, non_blocking=True)\n",
    "                val_attention_mask = vdata[\"attention_mask\"].to(DEVICE, non_blocking=True)\n",
    "                labels = vdata[\"label\"].to(DEVICE, non_blocking=True)\n",
    "                qids = vdata[\"query_ids\"]\n",
    "                dids = vdata[\"document_ids\"]\n",
    "\n",
    "                voutputs = model(val_input_tokens_ids, val_attention_mask)\n",
    "                vloss = loss_fn(voutputs, labels)\n",
    "                running_vloss += vloss.item()\n",
    "\n",
    "                scores = torch.sigmoid(voutputs).cpu().numpy()\n",
    "                for x, qid in enumerate(qids):\n",
    "                    if qid not in reranked_results:\n",
    "                        reranked_results[qid] = []\n",
    "                    reranked_results[qid].append((dids[x], float(scores[x])))\n",
    "\n",
    "        avg_vloss = running_vloss / (k + 1)\n",
    "        print('LOSS train {} valid {}'.format(last_loss, avg_vloss))\n",
    "\n",
    "        # sort by model score\n",
    "        for qid, doc_scores in reranked_results.items():\n",
    "            reranked_results[qid] = [doc for doc, _ in sorted(doc_scores, key=lambda x: x[1], reverse=True)]\n",
    "\n",
    "        validation_file = os.path.join(OUTPUT_DIR, 'validation_ranked_questions_model.jsonl')\n",
    "        with open(validation_file, 'w') as f:\n",
    "            for qid, docs in reranked_results.items():\n",
    "                entry = {\n",
    "                    \"query_id\": qid,\n",
    "                    \"retrieved_documents\": docs\n",
    "                }\n",
    "                f.write(ujson.dumps(entry) + '\\n')\n",
    "\n",
    "        print(\"nDCG@10 (Model):\", ndcg.compute_average_ndcg(questions_file_path=QUESTIONS_FILE, results_file_path=validation_file, k=10, print_out=False))\n",
    "\n",
    "        # Track best performance, and save the model's state\n",
    "        if avg_vloss < best_vloss:\n",
    "            best_vloss = avg_vloss\n",
    "            model_path = '../output/model/model_{}_{}.pt'.format(timestamp, epoch + 1)\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    return model, model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e4217b",
   "metadata": {},
   "source": [
    "## Create Model\n",
    "\n",
    "The CNNInteractionBasedModel is a PyTorch neural network model designed to rerank documents based on their relevance to a given query. It leverages convolutional layers to capture interactions between query and document embeddings.\n",
    "\n",
    "**Key Components**:\n",
    "\n",
    "- **Embedding Layer**: Converts token IDs into dense vectors. Supports loading pretrained embeddings (e.g., GloVe).\n",
    "- **Convolutional Layer**: Captures local interactions between query and document embeddings.\n",
    "- **Activation Function**: Applies ReLU activation to introduce non-linearity.\n",
    "- **Pooling Layer**: Aggregates features using adaptive max pooling.\n",
    "- **Fully Connected Layer**: Maps extracted features to a single relevance score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a8afff8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size is:  30522\n"
     ]
    }
   ],
   "source": [
    "# Embeddings + Model\n",
    "print(\"Vocabulary size is: \", len(bert_tokenizer))\n",
    "\n",
    "model = Model(vocab_size=len(bert_tokenizer))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4f5191",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "### Training Setup \n",
    "\n",
    "We configure the model training with the following hyperparameters:\n",
    "\n",
    "- **EPOCHS**: number of times that the model will see the entire training dataset.\n",
    "- **BATCH_SIZE**: training samples are processed in batches query–document pairs.\n",
    "- **LR**: learning rate controls how fast the optimizer updates the weights.\n",
    "\n",
    "The training will automatically use a **GPU (CUDA)** if available, otherwise it will fall back to the **CPU**.\n",
    "\n",
    "### Training the Model\n",
    "\n",
    "We call the `train_model` function, which:\n",
    "\n",
    "**1.** Loads the training dataset (queries, documents, labels).\n",
    "\n",
    "**2.** Prepares data batches with proper padding.\n",
    "\n",
    "**3.** Optimizes the model using **binary cross-entropy loss**, where:\n",
    "\n",
    "- **positive samples (label = 1)**: documents in the gold standard set for a query.\n",
    "- **negative samples (label = 0)**: retrieved documents not in the gold set.\n",
    "\n",
    "**4.** Runs the training loop for the defined number of epochs.\n",
    "\n",
    "**5.** Returns the trained model, ready for reranking/evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d2ab6c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training new model...\n",
      "EPOCH 1:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only integer tensors of a single element can be converted to an index",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m LR = \u001b[32m2e-5\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining new model...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m model, model_path = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLR\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 47\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, epochs, lr)\u001b[39m\n\u001b[32m     44\u001b[39m running_loss = \u001b[32m0.\u001b[39m\n\u001b[32m     45\u001b[39m last_loss = \u001b[32m0.\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_tokens_ids\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput_token_ids\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mattention_mask\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/GitHub/Search_engine/venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/GitHub/Search_engine/venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:764\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    763\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    765\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    766\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/GitHub/Search_engine/venv/lib/python3.13/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/GitHub/Search_engine/notebooks/../src/utils.py:53\u001b[39m, in \u001b[36mcollate_fn\u001b[39m\u001b[34m(batch)\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_gold_standard\u001b[39m(questions_file):\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     54\u001b[39m \u001b[33;03m    Load gold-standard documents from a file with structure:\u001b[39;00m\n\u001b[32m     55\u001b[39m \u001b[33;03m    {\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[33;03m      \"question\": \"Is erenumab effective for trigeminal neuralgia?\", \u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[33;03m      \"goldstandard_documents\": [\"PMID:36113495\"], \u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[33;03m      \"query_id\": \"63f73f1b33942b094c000008\"\u001b[39;00m\n\u001b[32m     59\u001b[39m \u001b[33;03m    }\u001b[39;00m\n\u001b[32m     60\u001b[39m \n\u001b[32m     61\u001b[39m \u001b[33;03m    Returns:\u001b[39;00m\n\u001b[32m     62\u001b[39m \u001b[33;03m        gold_data: dict mapping query_id -> {\u001b[39;00m\n\u001b[32m     63\u001b[39m \u001b[33;03m            \"question\": question_text,\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[33;03m            \"goldstandard_documents\": set_of_doc_ids\u001b[39;00m\n\u001b[32m     65\u001b[39m \u001b[33;03m        }\u001b[39;00m\n\u001b[32m     66\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m     67\u001b[39m     gold_data = {}\n\u001b[32m     68\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(questions_file, \u001b[33m'\u001b[39m\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[31mTypeError\u001b[39m: only integer tensors of a single element can be converted to an index"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "LR = 2e-5\n",
    "\n",
    "print(\"Training new model...\")\n",
    "model, model_path = train_model(model, epochs=EPOCHS, lr=LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d6f706",
   "metadata": {},
   "source": [
    "## Computing Ranking Metrics (BM25 Results) \n",
    "\n",
    "The system includes a script to compute the **Normalized Discounted Cumulative Gain (nDCG)** metric, which evaluates the quality of the ranked retrieval results. For this manner, execute the `nDCG.py` script.\n",
    "\n",
    "#### How nDCG Works\n",
    "\n",
    "- **DCG (Discounted Cumulative Gain)**: Measures the gain (relevance) of each document in the result list, discounted by its position in the list.\n",
    "- **IDCG (Ideal DCG)**: The maximum possible DCG achievable, obtained by an ideal ranking of documents.\n",
    "- **nDCG**: The ratio of DCG to IDCG, normalized to a value between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036068aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute nDCG for the given results\n",
    "ndcg.compute_average_ndcg(\n",
    "    questions_file_path=QUESTIONS_FILE,\n",
    "    results_file_path=BM25_OUPUT_FILE,\n",
    "    k=10\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366a0c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model\n",
    "model = Model(vocab_size=len(bert_tokenizer))\n",
    "model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
    "model.to(DEVICE)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2062e7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset for reranking\n",
    "dataset = PointWiseDataset(QUESTIONS_FILE, BM25_FILE, CORPUS_FILE, bert_tokenizer)\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=build_collate_fn(),\n",
    "    pin_memory=(DEVICE.type == \"cuda\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0851d3",
   "metadata": {},
   "source": [
    "## Reranking with the Neural Model\n",
    "\n",
    "1. **Batch scoring**  \n",
    "- For each batch from the DataLoader, we take the tokenized queries and candidate documents.  \n",
    "- The model outputs a **relevance score** for each (query, document) pair.  \n",
    "\n",
    "2. **Collect scores per query**  \n",
    "- We store the `(document_id, score)` pairs for every query.  \n",
    "\n",
    "3. **Sort candidates**  \n",
    "- For each query, we sort the candidate documents in descending order of model score.  \n",
    "- This step produces the final reranked list of documents for each query.  \n",
    "\n",
    "4. **Save results**  \n",
    "- The results are saved in a JSONL file with the format:\n",
    "   ```json\n",
    "   {\n",
    "      \"query_id\": \"...\",\n",
    "      \"retrieved_documents\": [\"doc1\", \"doc2\", \"doc3\", ...]\n",
    "   }\n",
    "   ```\n",
    "- This keeps the same structure as the BM25 file, making it easy to compare baseline vs reranked performance.  \n",
    "\n",
    "This reranking step does not retrieve new documents — it only **reorders the BM25 shortlist** according to the learned neural model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa1166a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run reranking\n",
    "reranked_results = {}\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in loader:\n",
    "        input_tokens_ids = batch[\"input_token_ids\"].to(DEVICE, non_blocking=True)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE, non_blocking=True)\n",
    "        qids = batch[\"query_ids\"]\n",
    "        dids = batch[\"document_ids\"]\n",
    "\n",
    "        scores = model(input_tokens_ids, attention_mask)\n",
    "        scores = torch.sigmoid(scores).cpu().numpy()\n",
    "\n",
    "        for i, qid in enumerate(qids):\n",
    "            if qid not in reranked_results:\n",
    "                reranked_results[qid] = []\n",
    "            reranked_results[qid].append((dids[i], float(scores[i])))\n",
    "\n",
    "# sort by model score\n",
    "for qid, doc_scores in reranked_results.items():\n",
    "    reranked_results[qid] = [doc for doc, _ in sorted(doc_scores, key=lambda x: x[1], reverse=True)]\n",
    "\n",
    "output_file = os.path.join(OUTPUT_DIR, 'ranked_questions_model.jsonl')\n",
    "with open(output_file, 'w') as f:\n",
    "    for qid, docs in reranked_results.items():\n",
    "        entry = {\n",
    "            \"query_id\": qid,\n",
    "            \"retrieved_documents\": docs\n",
    "        }\n",
    "        f.write(ujson.dumps(entry) + '\\n')\n",
    "\n",
    "print(f\"Reranked results saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a07e1ab",
   "metadata": {},
   "source": [
    "## Evaluate Retrieved Documents (Model Reranking)\n",
    "\n",
    "Here we compute the **Normalized Discounted Cumulative Gain (nDCG)** metric, which evaluates the quality of the ranked retrieval results after model reranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc602d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Reranked nDCG@10 (Model)\")\n",
    "\n",
    "# Compute nDCG after reranking\n",
    "ndcg.compute_average_ndcg(\n",
    "    questions_file_path=QUESTIONS_FILE,\n",
    "    results_file_path=output_file,\n",
    "    k=10\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
